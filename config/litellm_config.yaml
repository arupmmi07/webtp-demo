# LiteLLM Proxy Configuration
# This acts as a unified gateway for all LLM providers

model_list:
  # LM Studio - Local Models (FREE - No API key needed)
  - model_name: local-llama
    litellm_params:
      model: openai/local-model  # LM Studio uses OpenAI-compatible API
      api_base: http://host.docker.internal:1234/v1  # LM Studio default port
      api_key: "lm-studio"  # Any value works, LM Studio doesn't validate
      max_tokens: 4096
      temperature: 0.7
      
  - model_name: local-mistral
    litellm_params:
      model: openai/local-model
      api_base: http://host.docker.internal:1234/v1
      api_key: "lm-studio"
      max_tokens: 4096
      temperature: 0.7
      
  - model_name: gpt-oss-20b
    litellm_params:
      model: openai/gpt-oss-20b  # GPT-OSS-20B model
      api_base: http://host.docker.internal:1234/v1
      api_key: "lm-studio"
      max_tokens: 8192
      temperature: 0.7

  # Anthropic Claude Models (requires API key)
  - model_name: claude-sonnet-4
    litellm_params:
      model: anthropic/claude-sonnet-4-20250514
      api_key: os.environ/ANTHROPIC_API_KEY
      max_tokens: 8192
      temperature: 0.7

  - model_name: claude-sonnet-3.5
    litellm_params:
      model: anthropic/claude-3-5-sonnet-20241022
      api_key: os.environ/ANTHROPIC_API_KEY
      max_tokens: 8192
      temperature: 0.7

  - model_name: claude-haiku-3.5
    litellm_params:
      model: anthropic/claude-3-5-haiku-20241022
      api_key: os.environ/ANTHROPIC_API_KEY
      max_tokens: 8192
      temperature: 0.7

  # OpenAI Models (optional - requires API key)
  - model_name: gpt-4o
    litellm_params:
      model: openai/gpt-4o
      api_key: os.environ/OPENAI_API_KEY
      max_tokens: 4096
      temperature: 0.7

  - model_name: gpt-4o-mini
    litellm_params:
      model: openai/gpt-4o-mini
      api_key: os.environ/OPENAI_API_KEY
      max_tokens: 4096
      temperature: 0.7

# Router Configuration (for load balancing, fallbacks)
router_settings:
  routing_strategy: simple-shuffle  # Options: simple-shuffle, least-busy, latency-based-routing
  model_group_alias:
    local: ["gpt-oss-20b", "local-llama", "local-mistral"]  # FREE - LM Studio models
    production: ["claude-sonnet-4", "claude-sonnet-3.5"]
    fast: ["claude-haiku-3.5", "gpt-4o-mini"]
    fallback: ["gpt-4o", "claude-sonnet-3.5"]
    dev: ["gpt-oss-20b", "local-llama", "claude-haiku-3.5"]  # Local first, then cheap API

# General Settings
general_settings:
  # Store prompts and responses in database for tracing
  store_model_in_db: true
  store_prompts_in_spend_logs: true
  
litellm_settings:
  # Caching (disabled - redis not configured)
  cache: false
  # cache_params:
  #   type: redis
  #   host: redis
  #   port: 6379
    
  # Retry & Timeout
  # num_retries: How many times to retry on failure
  # request_timeout: Max seconds to wait for LLM response (default: 600 = 10 minutes)
  num_retries: 3
  request_timeout: 600  # Increase this if you get timeouts (e.g., 1200 for 20 min)
  
  # Note: Fallbacks removed - use model_group_alias in router_settings instead
  # This avoids configuration conflicts with LiteLLM's fallback format
  
  # Rate Limiting (per model) - TOTAL BUDGET: $5/day
  model_max_budget:
    claude-sonnet-4: 3.00      # $3/day (primary)
    claude-sonnet-3.5: 1.50    # $1.50/day (fallback)
    claude-haiku-3.5: 0.50     # $0.50/day (fast/cheap)
    gpt-4o: 1.00               # $1/day (fallback)
    gpt-4o-mini: 0.25          # $0.25/day (very cheap)
  
  # Logging
  success_callback: ["langfuse"]
  failure_callback: ["langfuse"]

# Callbacks (Observability)
callbacks:
  langfuse:
    langfuse_public_key: os.environ/LANGFUSE_PUBLIC_KEY
    langfuse_secret_key: os.environ/LANGFUSE_SECRET_KEY
    langfuse_host: os.environ/LANGFUSE_HOST

# Environment Variables
environment_variables:
  ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY}
  OPENAI_API_KEY: ${OPENAI_API_KEY}
  LANGFUSE_PUBLIC_KEY: ${LANGFUSE_PUBLIC_KEY}
  LANGFUSE_SECRET_KEY: ${LANGFUSE_SECRET_KEY}
  LANGFUSE_HOST: ${LANGFUSE_HOST}
