# LLM Provider Configuration
# Swap provider by changing the 'provider' field

provider: azure  # Options: azure, anthropic, openai, ollama, gemini

# Azure OpenAI Configuration (if using Azure OpenAI on GCP)
azure:
  model: ${AZURE_DEPLOYMENT_NAME}
  api_key: ${AZURE_API_KEY}
  api_base: ${AZURE_API_BASE}
  api_version: ${AZURE_API_VERSION:-2024-02-15-preview}
  max_tokens: 4096
  temperature: 0.7

# Anthropic (Claude) Configuration
anthropic:
  model: claude-sonnet-4-20250514
  api_key: ${ANTHROPIC_API_KEY}
  max_tokens: 4096
  temperature: 0.7

# OpenAI Configuration (future)
openai:
  model: gpt-4-turbo
  api_key: ${OPENAI_API_KEY}
  max_tokens: 4096
  temperature: 0.7

# Ollama (local) Configuration (future)
ollama:
  model: llama2
  base_url: http://localhost:11434
  max_tokens: 4096
  temperature: 0.7

# Gemini Configuration (future)
gemini:
  model: gemini-pro
  api_key: ${GEMINI_API_KEY}
  max_tokens: 4096
  temperature: 0.7

