# LLM Provider Configuration
# Swap provider by changing the 'provider' field

provider: ${ORCHESTRATION_LLM_PROVIDER:-azure}

# Azure OpenAI Configuration
azure:
  model: ${ORCHESTRATION_LLM_MODEL:-gpt-5-chat}
  api_key: ${ORCHESTRATION_LLM_AZURE_API_KEY}
  api_base: ${ORCHESTRATION_LLM_AZURE_ENDPOINT}
  api_version: ${ORCHESTRATION_LLM_AZURE_API_VERSION:-2025-03-01-preview}
  max_tokens: ${ORCHESTRATION_LLM_MAX_TOKENS:-2000}
  temperature: ${ORCHESTRATION_LLM_TEMPERATURE:-0.7}

# Anthropic (Claude) Configuration
anthropic:
  model: claude-sonnet-4-20250514
  api_key: ${ANTHROPIC_API_KEY}
  max_tokens: 4096
  temperature: 0.7

# OpenAI Configuration (future)
openai:
  model: gpt-4-turbo
  api_key: ${OPENAI_API_KEY}
  max_tokens: 4096
  temperature: 0.7

# Ollama (local) Configuration (future)
ollama:
  model: llama2
  base_url: http://localhost:11434
  max_tokens: 4096
  temperature: 0.7

# Gemini Configuration (future)
gemini:
  model: gemini-pro
  api_key: ${GEMINI_API_KEY}
  max_tokens: 4096
  temperature: 0.7

